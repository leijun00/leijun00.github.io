---
layout: post
title: 数学基础之概率
categories:
- math
tags:
- 数学
image:
    teaser: /math/probability.png
---


本文主要介绍概率与数理统计中的一些常见的基本概念. 

-------------------------------------------------




## 样本空间

对于随机试验, 尽管在每次试验之前不能预知试验的结果, 但是试验的所有可能结果集合是已知的, 我们将随机试验$$E$$的所有可能的结果组成的集合称为$$E$$的样本空间, 记为$$S$$. 样本空间的的元素, 即$$E$$的每个可能结果, 称为样本点. 比如事件$$E$$:抛一枚硬币, 观察正面$$H$$, 反面$$T$$出现的情况, $$S={H, T}$$. 




## 频率(Frequency) 概率(Probability)

频率描述了事件发生的频繁程度, 一般采用多次试验的结果得到. 
概率描述的是一次试验中, 事件发生的可能性大小. 
如果试验的次数足够多, 频率将在一定意义下接近于概率. 




## 条件概率(Conditional Probability)

设$$A, B$$是两个事件, 且$$P(A)>0$$, 称:

$$
P(B|A) = \frac {P(AB)}{P(A)}
$$ 

为事件$$A$$发生的条件下事件$$B$$发生的概率. 




## 乘法定理(Product rule)

设$$P(A)>0$$, 则:

$$
P(AB)=P(B|A)P(A)
$$

$$
P(ABC)=P(C|AB)P(B|A)P(A)
$$

这个定理也很容易推广到多个事件的情况




## 加法定理(Sum rule)

设试验$$E$$的样本空间为$$S$$, $$A$$为$$E$$的事件, $$B_1, B_2, \ldots, B_n$$为S的一个划分, 且 $$P(B_i)>0$$, 则:

$$
P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \ldots + P(A|B_n)P(B_n) 
$$




## 贝叶斯公式(Bayes' theorem)

$$
P(B_i|A) = \frac {P(A|B_i)P(B_i)}{\sum_{j=1}^{n}P(A|B_i)P(B_i)}
$$




## 先验概率(Prior probability) 后验概率(Posterior probability)

例子:某种设备, 调整良好时, 产品合格率为90%, 发生故障时, 合格率为30%, 每天早上开工时, 设备调整良好的概率为75%, 已知早上第一件产品是合格品, 问设备调整良好的概率是多少?如果定义事件$$A$$为产品合格, 事件$$B$$为设备调整良好, 显然有:

$$
P(A|B)=0. 9, P(A|\overline{b})=0. 3, P(B)=0. 75, P(\overline{B})=0. 25
$$
 
要求的是
$$
P(B|A)
$$
,$$P(B)$$称为先验概率, 是根据以往的经验数据得到的, 
$$
P(B|A)
$$是得到了第一件产品为合格品之后对$$P(B)$$做的修正, 称为后验概率, 后验概率让我们对设备的情况有了更进一步的了解. 




## 独立事件

如果A, B两个事件满足

$$
P(AB)=P(A)P(B)
$$

称A, B为互相独立的事件. 这个式子也很容易推广到多个事件的情况. 




## 随机变量

如果将随机试验的结果数量化, 比如抛硬币, 用 1 代表正面, 用 0 代表反面. 如果将这个数量化的结果用一个变量$$X$$表示, X就是随机变量, 根据实验结果的不同而不同. 正规的定义是:
设$$E$$是随机试验, 样本空间是$$S={e}$$, 如果对于每一个$$e$$属于$$S$$, 都有一个实数$$X(e)$$与之对应, 这样就得到一个定义在$$S$$上的单值函数$$X=X(e)$$, 称为随机变量. 如果$$X$$能取到的值是有限个或者可列无限个, 则$$X$$称为离散性随机变量. 




## 概率分布

如果离散性随机变量$$X$$的所有取值为 $$x_k(k=1, 2, . . . )$$, $$X$$取各个值得概率为:

$$
P\{ X=x_k \}=p_k
$$

称为离散性随机变量X的概率分布或者分布律. 




##分布函数(Cumulative distribution fucntion)

对于非离散性随机变量X, 其可能的取值不能一一列举出来, 所以不能用像离散性随机变量那样用分布律来吗描述, 为此引入随机变量分布函数的概率. 

设$$X$$是一随机变量, $$x$$是任意实数, 函数:

$$
F(x) = P \{ X \leq x \}
$$

称为$$X$$的分布函数. 虽然对离散性随机变量, 可以完全用分布律来描述, 但为了数学上的统一, 定义了对离散性随机变量和非离散性随机变量都适用的分布函数. 




## 连续性随机变量 概率密度(Probability density function)

如果随机变量X的分布函数是$$F(x)$$, 存在非负函数$$f(x)$$, 使得对于任意实数$$x$$有:

$$
F(x) = \int_{-\infty}^{x}f(t)dt 
$$

则称$$X$$为连续性随机变量, $$f(x)$$称为$$X$$的概率密度函数, 简称概率密度. 

概率密度具有以下性质:

$$
f(x) \geq 0 
$$

$$
\int_{-\infty}^{\infty}f(x)dx = 1
$$

$$
P \{ x_1 < X \leq x_2 \} = F(x_2) - F(x_1) = \int_{x_1}^{x_2}f(x)dx 
$$




## 期望(Expectation)

设离散性随机变量$$X$$的分布律为:

$$
P\{ X=x_k \}=p_k
$$

如果级数

$$
\sum_{k=1}^{\infty}x_k p_k 
$$

绝对收敛, 则称为随机变量$$X$$的期望, 记作$$E(X)$$. 

对于连续性随机变量X的概率密度为$$f(x)$$,  期望为:

$$
\int_{-\infty}^{\infty}xf(x)dx
$$

如果有函数$$Y=g(x)$$, 则Y的期望为:

$$
\int_{-\infty}^{\infty}g(x)f(x)dx
$$

期望又称均值. 




## 方差(Variance)

设$$X$$是一个随机变量, 如果$$E\{[X-E(X)]^2\}$$存在, 则称为X的方差, 记为$$D(X)$$或者$$Var(X)$$. 

方差可以按照公式 $$D(X) = E(X^2) - [E(X)]^2 $$计算. 

方差开方$$\sqrt {D(x)}$$记为 $$\sigma(X)$$, 称为标准差或者均方差. 




## 矩

设$$X$$是随机变量

$$X$$的$$k$$阶原点矩:

$$
E(X^k)
$$

$$X$$的$$k$$阶中心矩:

$$
E\{ [X-E(X)]^k\}
$$

显然$$X$$的期望是$$X$$的一阶原点矩, 方差是$$X$$的二阶中心矩




## 常见概率分布



### 0-1分布 伯努利分布(Bernoulli distribution)

离散性随机变量的概率分布, 随机变量$$X$$只能取0和1两个值, 它的分布律是

$$
P\{ X=k \} = p^k(1-p)^{1-k},  k=0, 1
$$

$$
E(X) = p$$,  $$D(X) = p(1-p)
$$



### 二项分布(Binomial distribution)

随机变量X表示n重伯努利试验中事件$$A$$发生的次数, 例如重复抛$$n$$次硬币, 出现正面的次数, $$X$$的分布律是:

$$
P\{ X=k \} = {n \choose k}p^k(1-p)^{n-k},  k=0, 1, 2, . . . , n
$$

$$
E(X) = np$$,  $$D(X) = np(1-p)
$$



### 泊松分布(Poisson distribution)

设随机变量$$X$$所有的可能取值为0, 1, 2, . . . , 而取各个值得概率为

$$
P\{ X=k \} = \frac {\lambda^k e^{-\lambda}}{k!},  k=0, 1, 2, . . . 
$$

则称$$X$$服从参数为 $$\lambda$$的泊松分布. 

$$
E(X) = \lambda$$,  $$D(X) = \lambda
$$

在实际事例中, 当一个随机事件, 以固定的平均瞬时速率λ(或称密度)随机且独立地出现时, 那么这个事件在单位时间(面积或体积)内出现的次数或个数就近似地服从泊松分布. 因此, 泊松分布在管理科学、运筹学以及自然科学的某些问题中都占有重要的地位. 例如:

- 某一服务设施在一定时间内到达的人数
- 电话交换机接到呼叫的次数
- 汽车站台的候客人数
- 机器出现的故障数
- 自然灾害发生的次数
- 一本书一页中的印刷错误
- 显微镜下单位分区内的细菌分布数
- 某放射性物质单位时间发射出的粒子数
- 某地区一天内丢失的邮件数
- 某医院一天内的急诊人数



### 均匀分布(Uniform distribution)

设连续性随机变量$$X$$具有概率密度

$$
f(x) = \left \{  {\frac {1} {b-a},  \qquad a<x<b,  \atop 0,  \qquad  \text{其他}} \right. 
$$

则称$$X$$在区间$$[a, b]$$上服从均匀分布.

$$
E(X)=\frac {a+b}{2}$$,  $$D(X)=\frac {(b-a)^2}{12}
$$



### 正态分布(Normal distribution,  Gaussian distribution)

设连续性随机变量$$X$$的概率密度为:

$$
f(x) = \frac {1}{\sqrt {2 \pi \sigma^2}} e^{-\frac {(x-\mu)^2}{2 \sigma^2}},  -\infty < x < \infty
$$

则称$$X$$服从参数为$$\mu$$,  $$\sigma$$的正态分布, 正态分布又叫高斯分布. 

$$
E(X)=\mu$$,  $$D(X)=\sigma^2
$$




## 大数定理

随机试验中, 随着试验次数的增加, 人们发现事件发生的频率逐渐稳定于某个常数(想想抛硬币的例子), 在实践中, 人们还认识到大量测量值的算数平均值也具有稳定性, 这种稳定性就是大数定理的客观背景. 这里我们介绍其中的一个大数定理:



### 辛钦定理

设随机变量$$X_1, X_2, \ldots, X_n$$相互独立, 服从同一分布(independent and identically distributed,  i. i. d. ), 且具有相同的数学期望, $$E(X_k)=\mu$$, 则:

$$
\lim_{n \to \infty} P \{ |\frac {1} {n} \sum_{k=1}^{n} X_k - \mu |<\varepsilon \} = 1
$$




## 中心极限定理

在客观实际中有许多随机变量, 他们是由大量相互独立的随机因素的综合影响形成的, 而其中每一个个别因素在总的影响中所起的作用都是微小的, 这种随机变量往往近似地服从正态分布, 这种现象就是中心极限定理的客观背景. 这里只介绍独立同分布的中心极限定理. 



### 独立同分布的中心极限定理

设随机变量$$X_1, X_2, \ldots, X_n$$相互独立, 服从同一分布, 且具有相同的数学期望, $$E(X_k)=\mu$$ 和相同的方差 $$D(X_k)=\sigma^2 \neq 0$$, 则随机变量:

$$
Y_n = \frac {\sum_{k=1}^{n} X_k - E(\sum_{k=1}^{n} X_k)}{\sqrt {D(\sum_{k=1}^{n} X_k)}} = \frac {\sum_{k=1}^{n} X_k - n\mu}{\sqrt{n}\sigma}
$$

在$$n$$很大时趋近于标准正态分布. 

当这些随机变量不是服从同一分布的时候, 他们的和在$$n$$很大时仍然服从正态分布, 这就是正态分布为什么概率中特别重要的原因. 在很多问题中, 所考虑的随机变量可以表示成很多独立的随机变量之和, 例如, 在任一指定时刻, 一个城市的耗电量是大量用户的耗电量的总和, 一个物理实验的测量误差是许多观察不到的, 可加的微小误差所合成的, 他们往往近似的服从正态分布. 




## 参数估计



### 点估计

设总体X的分布函数形式已知, 但有一个或者多个未知参数, 借助于总体X的一个样本来估计总体未知参数的值的问题称为参数的点估计问题. 常用的点估计方法有矩估计法和最大似然估计法. 

例子:设总体 $$X$$ 的均值 $$\mu$$ 和方差 $$\sigma^2$$均未知, 已知$$X_1, X_2, \ldots, X_n$$ 是一个样本, 估计均值 $$\mu$$ 和方差 $$\sigma^2$$



### 矩估计

分别计算样本矩和总体矩的前k阶矩, 利用样本矩依概率收敛于总体矩的性质, 构造相应的方程组, 用方程组的解作为参数的估计量, 这时候的估计量称为矩估计量. 

用矩估计法解上面的例子:

易知总体矩:

$$
\mu_1 = E(X) = \mu 
$$

$$
\mu_2 = E(X^2) = D(X) + [E(X)]^2 = \mu^2 + \sigma^2 
$$

计算样本矩:

$$
A_1 = \frac {1}{n} \sum_{i=1}^n X_i = \overline {X} 
$$

$$
A_2 = \frac {1}{n} \sum_{i=1}^n X_i^2 
$$

联立方程组

$$
A_1 = \mu_1 
$$

$$
A_2 = \mu_2 
$$

解得:

$$
\hat{\mu} = \overline {X} 
$$

$$
\hat {\sigma^2} = \frac {1}{n} \sum_{i=1}^{n}(X_i - \overline {X})^2
$$



### 最大似然估计(Maximum likelihood)

设总体$$X$$属于离散性, 其分布律为 $$P(X=x)=p(x;\theta)$$, 形式已知, 但参数$$\theta$$未知. 已知$$X_1, X_2, \ldots, X_n$$ 是一个样本, 则$$X_1, X_2, \ldots, X_n$$的联合分布律为:

$$
\Pi_{i=1}^{n}p(x_i;\theta)
$$

设$$x_1, x_2, \ldots, x_n$$是相应于样本$$X_1, X_2, \ldots, X_n$$的一个样本值, 已知样本取到$$x_1, x_2, \ldots, x_n$$的概率为, 也即事件 $$\{ X_1=x_1,  X_2 = x_2,  \ldots,  X_n = x_n\}$$ 发生的概率为:

$$
L(\theta) = L(x_1, x_2, \ldots, x_n;\theta) = \Pi_{i=1}^{n}p(x_i;\theta) 
$$

这一概率随 $$\theta$$的变化而变化, 是$$\theta$$的函数, 称为样本的似然函数. 

用使似然函数取得最大值的$$\theta$$作为原分布律未知参数的估计值, 称为极大似然估计值. 

当总体$$X$$属于连续型时, 考虑的是样本$$X_1, X_2, \ldots, X_n$$ 落到$$x_1, x_2, \ldots, x_n$$ 的领域内的概率, 和离散性的表达形式一样. 

用最大似然估计解上面的例子

$$X$$的概率密度为:

$$
f(x; \mu, \sigma^2) = \frac {1}{\sqrt {2 \pi \sigma^2}} e^{-\frac {(x-\mu)^2}{2 \sigma^2}}
$$

似然函数为:

$$
L(\mu,  \sigma^2)=\Pi_{i=1}^{n} \frac {1}{\sqrt {2 \pi \sigma^2}} e^{-\frac {(x-\mu)^2}{2 \sigma^2}}
$$

取对数, 然后分别对 $$\mu$$,  $$\sigma^2$$求偏导数, 并令偏导数为0, 解得:

$$
\hat{\mu} = \overline {X} 
$$

$$
\hat {\sigma^2} = \frac {1}{n} \sum_{i=1}^{n}(X_i - \overline {X})^2
$$

和用矩估计法求得的估计值完全相同. 



### 估计量的评选标准

评价一个估计量的好坏, 有很多常用的标准, 这里只介绍最常用的两个标准, 无偏性和有效性. 


#### 无偏性

如果估计量$$\hat {\theta}=\hat {\theta}(X_1,  X_2,  \ldots,  X_n)$$的期望存在, 而且有:

$$
E(\hat{\theta}) = \theta 
$$

则称$$\hat {\theta}$$为$$\theta$$的无偏估计量. 

检验上面例子中的估计值:

$$
E(\hat {\sigma^2}) = \frac {n-1}{n} \sigma^2 \neq \sigma^2 
$$

所以估计量$$\hat {\theta}$$是有偏的. 


#### 有效性

设估计量$$\hat {\theta_1}=\hat {\theta_1}(X_1,  X_2,  \ldots,  X_n)$$和估计量$$\hat {\theta_2}=\hat {\theta_2}(X_1,  X_2,  \ldots,  X_n)$$都是$$\theta$$的无偏估计量, 如果:

$$
D(\hat {\theta_1}) < D(\hat {\theta_2})
$$

则称 $$\hat {\theta_1} $$比$$\hat {\theta_2} $$有效. 




## 练习题

最后附上CMU的一套简单[测试题][1], 可以用来你是否具备学习机器学习入门的数学基础. 




## 参考资料

1. 概率论与数理统计 高等教育出版社
2. Pattern Recognition and Machine Learning Chapter1,Chapter2,Appendix B

  [1]: http://www.cs.cmu.edu/~aarti/Class/10701_Spring14/Intro_ML_Self_Evaluation.pdf

